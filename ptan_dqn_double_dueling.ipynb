{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DQN using PTAN library\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning) \n",
    "# suppress numpy future warnings \n",
    "# warning created due to issues with tensorflow 1.14\n",
    "\n",
    "\n",
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from lib import dqn_model, common\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable GPU [default:False]\")\n",
    "# parser.add_argument(\"--seed\", type=int, help=\"Set seed [default: 42]\")\n",
    "# parser.add_argument(\"experiment\", help=\"Experiment to run. Specified in ./lib/common.py\")\n",
    "\n",
    "\n",
    "\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE # cuda:0 :  GeForce RTX 2080 SUPER\n"
     ]
    }
   ],
   "source": [
    "# n-step rollout\n",
    "ROLLOUT_STEPS = 1\n",
    "\n",
    "# Double DQN flag\n",
    "DOUBLE = True\n",
    "# evaluate Q-values of random states\n",
    "NO_OF_STATES_TO_EVALUATE = 1000 # how many states to sample to evaluate\n",
    "EVAL_FREQ = 100 # how often to evaluate\n",
    "\n",
    "# Dueling Networks\n",
    "DUELING=True\n",
    "\n",
    "# set device\n",
    "USE_GPU = True#args.cuda\n",
    "USE_CUDA = torch.cuda.is_available() and USE_GPU\n",
    "device = torch.device(\"cuda:0\" if USE_CUDA else \"cpu\")\n",
    "print(\"DEVICE #\", device, \": \", torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 324267 #args.seed if args.seed else 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "experiment = \"pong\"#args.experiment\n",
    "params = common.HYPERPARAMS[experiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(params['env_name'])\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "\n",
    "nstep_tag = str(ROLLOUT_STEPS)+\"step\"\n",
    "tag = params['run_name'] + '-' + nstep_tag  # nstep rollouts\n",
    "if DOUBLE:\n",
    "    double_tag = 'double'\n",
    "    tag = tag + '-' + double_tag # double dqn\n",
    "\n",
    "if DUELING:\n",
    "    dueling_tag = 'dueling'\n",
    "    tag = tag + '-' + dueling_tag # double dqn\n",
    "    \n",
    "tag = tag + '-'+ str(seed) # seed\n",
    "writer = SummaryWriter(comment=\"-\"+ tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dueling Networks\n",
    "if DUELING:\n",
    "    net = dqn_model.DuelingDQN(env.observation_space.shape, \n",
    "                    env.action_space.n).to(device)\n",
    "    \n",
    "net = dqn_model.DQN(env.observation_space.shape, \n",
    "                    env.action_space.n).to(device)\n",
    "\n",
    "\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])\n",
    "epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "# EpsilonTracker has only one method frame(frame_idx) which changes the epsilon value \n",
    "# of selector=>ptan.actions.EpsilonGreedyActionSelector depending upon the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=ROLLOUT_STEPS)\n",
    "\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=params['replay_size'])\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2e7f66c607f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mloss_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_loss_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mROLLOUT_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1E3\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/ptan_dqn/lib/common.py\u001b[0m in \u001b[0;36mcalc_loss_dqn\u001b[0;34m(batch, net, tgt_net, gamma, device, double)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# returns numpy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mstates_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0mnext_states_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mactions_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frame_idx = 0\n",
    "eval_states = None # will be populated with held-out states\n",
    "\n",
    "with common.RewardTracker(writer, params['stop_reward']) as reward_tracker: #create a reward tracker object\n",
    "    while True:\n",
    "        frame_idx += 1\n",
    "        # ExperienceReplayBuffer asks the ExperienceSourceFirstLast to iterate by one step to get the next transition\n",
    "        # ExperienceSourceFirstLast feeds observation to obtain action\n",
    "        # Agent calculated Q-values through the NN\n",
    "        # Action selector selects action\n",
    "        # Action is fed into ExperienceSource to obtain reward and next obs\n",
    "        # Buffer stores transition in FIFO order\n",
    "        buffer.populate(1) # iterates ExperienceReplayBuffer by 1 step.\n",
    "                            # this in turn iterates exp_source [ExperienceSourceFirstLast] by one step\n",
    "                            # one single experience step\n",
    "                            # Experience = namedtuple('Experience', ['state', 'action', 'reward', 'done'])\n",
    "                            \n",
    "                            # Class ExperienceSource provides us full subtrajectories of given length as the list of (s, a, r, s') objects.\n",
    "                            # Now it returns single object on every iteration, which is again a namedtuple with the following fields:\n",
    "\n",
    "                            # state: state which we used to decide on action to make TYPE: numpy\n",
    "                            # action: action we've done at this step\n",
    "                            # reward: partial accumulated reward for steps_count (in our case, steps_count=1, so it is equal to immediate reward)\n",
    "                            # last_state: the state we've got after executing the action. If our episode ends, we have None here\n",
    "\n",
    "                            # For every trajectory piece it calculates discounted reward and emits only first and last states and action taken in the first state.\n",
    "        epsilon_tracker.frame(frame_idx)\n",
    "        \n",
    "\n",
    "        new_rewards = exp_source.pop_total_rewards() # get rewards from the episodes\n",
    "        if new_rewards:\n",
    "            if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):\n",
    "                break\n",
    "\n",
    "        if len(buffer) < params['replay_initial']:\n",
    "            continue\n",
    "            \n",
    "        # evaluate states\n",
    "        if eval_states is None:\n",
    "                eval_states = buffer.sample(NO_OF_STATES_TO_EVALUATE)\n",
    "                eval_states = [np.array(transition.state, copy=False) for transition in eval_states]\n",
    "                eval_states = np.array(eval_states, copy=False)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(params['batch_size'])\n",
    "        loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model, gamma=params['gamma']**ROLLOUT_STEPS, device=device)\n",
    "        if frame_idx % 1E3 == 0:\n",
    "            writer.add_scalar(\"loss\", loss_v, frame_idx)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if frame_idx % params['target_net_sync'] == 0:\n",
    "            tgt_net.sync()\n",
    "            \n",
    "        if frame_idx % EVAL_FREQ == 0:\n",
    "                mean_val = common.calc_values_of_states(eval_states, net, device=device)\n",
    "                writer.add_scalar(\"values_mean\", mean_val, frame_idx)\n",
    "            \n",
    "        if frame_idx > 3E6:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "cur_folder = os.getcwd()\n",
    "model_folder = os.path.join(cur_folder,\"models\")\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "model_file = os.path.join(model_folder, (tag + \".pt\"))\n",
    "torch.save(net.state_dict(), model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
