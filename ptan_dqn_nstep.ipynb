{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DQN using PTAN library\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning) \n",
    "# suppress numpy future warnings \n",
    "# warning created due to issues with tensorflow 1.14\n",
    "\n",
    "\n",
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from lib import dqn_model, common\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable GPU [default:False]\")\n",
    "# parser.add_argument(\"--seed\", type=int, help=\"Set seed [default: 42]\")\n",
    "# parser.add_argument(\"experiment\", help=\"Experiment to run. Specified in ./lib/common.py\")\n",
    "\n",
    "\n",
    "\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE # cuda:0 :  GeForce RTX 2080 SUPER\n"
     ]
    }
   ],
   "source": [
    "# n-step rollout\n",
    "ROLLOUT_STEPS = 2\n",
    "\n",
    "# set device\n",
    "USE_GPU = True#args.cuda\n",
    "USE_CUDA = torch.cuda.is_available() and USE_GPU\n",
    "device = torch.device(\"cuda:0\" if USE_CUDA else \"cpu\")\n",
    "print(\"DEVICE #\", device, \": \", torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 324267 #args.seed if args.seed else 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "experiment = \"beamrider-v0\"#args.experiment\n",
    "params = common.HYPERPARAMS[experiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(params['env_name'])\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "\n",
    "nstep_tag = str(ROLLOUT_STEPS)+\"step\"\n",
    "tag = params['run_name'] + '-' + nstep_tag + '-'+ str(seed)\n",
    "writer = SummaryWriter(comment=\"-\"+ tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = dqn_model.DQN(env.observation_space.shape, \n",
    "                    env.action_space.n).to(device)\n",
    "\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])\n",
    "epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "# EpsilonTracker has only one method frame(frame_idx) which changes the epsilon value \n",
    "# of selector=>ptan.actions.EpsilonGreedyActionSelector depending upon the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=ROLLOUT_STEPS)\n",
    "\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=params['replay_size'])\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 0\n",
    "with common.RewardTracker(writer, params['stop_reward']) as reward_tracker: #create a reward tracker object\n",
    "    while True:\n",
    "        frame_idx += 1\n",
    "        # ExperienceReplayBuffer asks the ExperienceSourceFirstLast to iterate by one step to get the next transition\n",
    "        # ExperienceSourceFirstLast feeds observation to obtain action\n",
    "        # Agent calculated Q-values through the NN\n",
    "        # Action selector selects action\n",
    "        # Action is fed into ExperienceSource to obtain reward and next obs\n",
    "        # Buffer stores transition in FIFO order\n",
    "        buffer.populate(1) # iterates ExperienceReplayBuffer by 1 step.\n",
    "                            # this in turn iterates exp_source [ExperienceSourceFirstLast] by one step\n",
    "                            # one single experience step\n",
    "                            # Experience = namedtuple('Experience', ['state', 'action', 'reward', 'done'])\n",
    "                            \n",
    "                            # Class ExperienceSource provides us full subtrajectories of given length as the list of (s, a, r, s') objects.\n",
    "                            # Now it returns single object on every iteration, which is again a namedtuple with the following fields:\n",
    "\n",
    "                            # state: state which we used to decide on action to make TYPE: numpy\n",
    "                            # action: action we've done at this step\n",
    "                            # reward: partial accumulated reward for steps_count (in our case, steps_count=1, so it is equal to immediate reward)\n",
    "                            # last_state: the state we've got after executing the action. If our episode ends, we have None here\n",
    "\n",
    "                            # For every trajectory piece it calculates discounted reward and emits only first and last states and action taken in the first state.\n",
    "        epsilon_tracker.frame(frame_idx)\n",
    "        \n",
    "\n",
    "        new_rewards = exp_source.pop_total_rewards() # get rewards from the episodes\n",
    "        if new_rewards:\n",
    "            if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):\n",
    "                break\n",
    "\n",
    "        if len(buffer) < params['replay_initial']:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(params['batch_size'])\n",
    "        loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model, gamma=params['gamma']**ROLLOUT_STEPS, device=device)\n",
    "        if frame_idx % 1E3 == 0:\n",
    "            writer.add_scalar(\"loss\", loss_v, frame_idx)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if frame_idx % params['target_net_sync'] == 0:\n",
    "            tgt_net.sync()\n",
    "            \n",
    "        if frame_idx > 3E6:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "cur_folder = os.getcwd()\n",
    "model_folder = os.path.join(cur_folder,\"models\")\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "model_file = os.path.join(model_folder, (tag + \".pt\"))\n",
    "torch.save(net.state_dict(), model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
