{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DQN using PTAN library\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning) \n",
    "# suppress numpy future warnings \n",
    "# warning created due to issues with tensorflow 1.14\n",
    "\n",
    "\n",
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from lib import dqn_model, common\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable GPU [default:False]\")\n",
    "# parser.add_argument(\"--seed\", type=int, help=\"Set seed [default: 42]\")\n",
    "# parser.add_argument(\"experiment\", help=\"Experiment to run. Specified in ./lib/common.py\")\n",
    "\n",
    "\n",
    "\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate Q-values of random states\n",
    "NO_OF_STATES_TO_EVALUATE = 1000 # how many states to sample to evaluate\n",
    "EVAL_FREQ = 100 # how often to evaluate\n",
    "\n",
    "# n-step rollout\n",
    "ROLLOUT_STEPS = 1\n",
    "\n",
    "# Double DQN flag\n",
    "DOUBLE = False\n",
    "\n",
    "\n",
    "# Dueling Networks\n",
    "DUELING=False\n",
    "\n",
    "# Feature Regularization\n",
    "SRG=False\n",
    "SRG_RATIO=1E-4\n",
    "\n",
    "# set device\n",
    "USE_GPU = True#args.cuda\n",
    "USE_CUDA = torch.cuda.is_available() and USE_GPU\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"DEVICE #\", device, \": \", torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 324267 #args.seed if args.seed else 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "experiment = \"pong\"#args.experiment\n",
    "params = common.HYPERPARAMS[experiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(params['env_name'])\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "\n",
    "nstep_tag = str(ROLLOUT_STEPS)+\"step\"\n",
    "tag = params['run_name'] + '-' + nstep_tag  # nstep rollouts\n",
    "if DOUBLE:\n",
    "    double_tag = 'double'\n",
    "    tag = tag + '-' + double_tag # double dqn\n",
    "\n",
    "if DUELING:\n",
    "    dueling_tag = 'dueling'\n",
    "    tag = tag + '-' + dueling_tag # double dqn\n",
    "\n",
    "if SRG:\n",
    "    srg_tag = 'srg_'+ str(SRG_RATIO)\n",
    "    tag = tag + '-'+ srg_tag    \n",
    "tag = tag + '-'+ str(seed) # seed\n",
    "writer = SummaryWriter(comment=\"-\"+ tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Regularized Networks\n",
    "if SRG:\n",
    "    if DUELING:\n",
    "        net = dqn_model.DuelingDQN_srg(env.observation_space.shape, \n",
    "                        env.action_space.n).to(device)\n",
    "    else:\n",
    "        net = dqn_model.DQN_srg(env.observation_space.shape, \n",
    "                            env.action_space.n).to(device)\n",
    "else:\n",
    "    if DUELING:\n",
    "        net = dqn_model.DuelingDQN(env.observation_space.shape, \n",
    "                        env.action_space.n).to(device)\n",
    "    else:\n",
    "        net = dqn_model.DQN(env.observation_space.shape, \n",
    "                            env.action_space.n).to(device)\n",
    "    \n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])\n",
    "epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "# EpsilonTracker has only one method frame(frame_idx) which changes the epsilon value \n",
    "# of selector=>ptan.actions.EpsilonGreedyActionSelector depending upon the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent_srg(ptan.agent.BaseAgent):\n",
    "    \"\"\"\n",
    "    DQNAgent is a memoryless DQN agent which calculates Q values\n",
    "    from the observations and  converts them into the actions using action_selector\n",
    "    \"\"\"\n",
    "    def __init__(self, dqn_model, action_selector, device=\"cpu\", preprocessor=ptan.agent.default_states_preprocessor):\n",
    "        self.dqn_model = dqn_model\n",
    "        self.action_selector = action_selector\n",
    "        self.preprocessor = preprocessor\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, states, agent_states=None):\n",
    "        if agent_states is None:\n",
    "            agent_states = [None] * len(states)\n",
    "        if self.preprocessor is not None:\n",
    "            states = self.preprocessor(states)\n",
    "            if torch.is_tensor(states):\n",
    "                states = states.to(self.device)\n",
    "        q_v, _ = self.dqn_model(states)\n",
    "        q = q_v.data.cpu().numpy()\n",
    "        actions = self.action_selector(q)\n",
    "        return actions, agent_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SRG:\n",
    "    agent = DQNAgent_srg(net, selector, device=device)\n",
    "else:\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=ROLLOUT_STEPS)\n",
    "\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=params['replay_size'])\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "frame_idx = 0\n",
    "eval_states = None # will be populated with held-out states\n",
    "\n",
    "with common.RewardTracker(writer, params['stop_reward']) as reward_tracker: #create a reward tracker object\n",
    "    while True:\n",
    "        frame_idx += 1\n",
    "        # ExperienceReplayBuffer asks the ExperienceSourceFirstLast to iterate by one step to get the next transition\n",
    "        # ExperienceSourceFirstLast feeds observation to obtain action\n",
    "        # Agent calculated Q-values through the NN\n",
    "        # Action selector selects action\n",
    "        # Action is fed into ExperienceSource to obtain reward and next obs\n",
    "        # Buffer stores transition in FIFO order\n",
    "        buffer.populate(1) # iterates ExperienceReplayBuffer by 1 step.\n",
    "                            # this in turn iterates exp_source [ExperienceSourceFirstLast] by one step\n",
    "                            # one single experience step\n",
    "                            # Experience = namedtuple('Experience', ['state', 'action', 'reward', 'done'])\n",
    "                            \n",
    "                            # Class ExperienceSource provides us full subtrajectories of given length as the list of (s, a, r, s') objects.\n",
    "                            # Now it returns single object on every iteration, which is again a namedtuple with the following fields:\n",
    "\n",
    "                            # state: state which we used to decide on action to make TYPE: numpy\n",
    "                            # action: action we've done at this step\n",
    "                            # reward: partial accumulated reward for steps_count (in our case, steps_count=1, so it is equal to immediate reward)\n",
    "                            # last_state: the state we've got after executing the action. If our episode ends, we have None here\n",
    "\n",
    "                            # For every trajectory piece it calculates discounted reward and emits only first and last states and action taken in the first state.\n",
    "        epsilon_tracker.frame(frame_idx)\n",
    "        \n",
    "\n",
    "        new_rewards = exp_source.pop_total_rewards() # get rewards from the episodes\n",
    "        if new_rewards:\n",
    "            if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):\n",
    "                break\n",
    "\n",
    "        if len(buffer) < params['replay_initial']:\n",
    "            continue\n",
    "            \n",
    "        # evaluate states\n",
    "#         if eval_states is None:\n",
    "#                 eval_states = buffer.sample(NO_OF_STATES_TO_EVALUATE)\n",
    "#                 eval_states = [np.array(transition.state, copy=False) for transition in eval_states]\n",
    "#                 eval_states = np.array(eval_states, copy=False)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(params['batch_size'])\n",
    "        if SRG: #state regularization\n",
    "            loss_v, feature_loss, qvalue_loss = common.calc_loss_srg_ratio(batch, net, tgt_net.target_model,\n",
    "                                                                           gamma=params['gamma']**ROLLOUT_STEPS, \n",
    "                                                                           device=device, \n",
    "                                                                           double=DOUBLE,\n",
    "                                                                           loss_ratio=SRG_RATIO)\n",
    "            if frame_idx % 1E3 == 0:\n",
    "                writer.add_scalar(\"loss\", loss_v, frame_idx)\n",
    "                writer.add_scalar(\"feature_loss\", feature_loss, frame_idx)\n",
    "                writer.add_scalar(\"qvalue_loss\", qvalue_loss, frame_idx)\n",
    "        else:\n",
    "            loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model, \n",
    "                                          gamma=params['gamma']**ROLLOUT_STEPS,\n",
    "                                          device=device,\n",
    "                                          double=DOUBLE)\n",
    "            if frame_idx % 1E3 == 0:\n",
    "                writer.add_scalar(\"loss\", loss_v, frame_idx)\n",
    "                \n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if frame_idx % params['target_net_sync'] == 0:\n",
    "            tgt_net.sync()\n",
    "            \n",
    "#         if frame_idx % EVAL_FREQ == 0:\n",
    "#             if SRG:\n",
    "#                 mean_val = common.calc_values_of_states_srg(eval_states, net, device=device)\n",
    "#             else:\n",
    "#                 mean_val = common.calc_values_of_states(eval_states, net, device=device)\n",
    "\n",
    "#             writer.add_scalar(\"values_mean\", mean_val, frame_idx)\n",
    "            \n",
    "        if frame_idx > 1E6:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "cur_folder = os.getcwd()\n",
    "model_folder = os.path.join(cur_folder,\"models\")\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "model_file = os.path.join(model_folder, (tag + \".pt\"))\n",
    "torch.save(net.state_dict(), model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
