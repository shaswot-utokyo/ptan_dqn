{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DQN using PTAN library\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning) \n",
    "# suppress numpy future warnings \n",
    "# warning created due to issues with tensorflow 1.14\n",
    "\n",
    "\n",
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from lib import dqn_model, common\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable GPU [default:False]\")\n",
    "# parser.add_argument(\"--seed\", type=int, help=\"Set seed [default: 42]\")\n",
    "# parser.add_argument(\"experiment\", help=\"Experiment to run. Specified in ./lib/common.py\")\n",
    "\n",
    "\n",
    "\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "USE_GPU = True#args.cuda\n",
    "USE_CUDA = torch.cuda.is_available() and USE_GPU\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42#args.seed if args.seed else 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "experiment = \"pong\"#args.experiment\n",
    "params = common.HYPERPARAMS[experiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(params['env_name'])\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "\n",
    "tag = params['run_name'] + \"-basic_srg\" + '_'+ str(seed)\n",
    "writer = SummaryWriter(comment=\"-\"+ tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = dqn_model.DQN_A(env.observation_space.shape, \n",
    "                    env.action_space.n).to(device)\n",
    "\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])\n",
    "epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "# EpsilonTracker has only one method frame(frame_idx) which changes the epsilon value \n",
    "# of selector=>ptan.actions.EpsilonGreedyActionSelector depending upon the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent_srg(ptan.agent.BaseAgent):\n",
    "    \"\"\"\n",
    "    DQNAgent is a memoryless DQN agent which calculates Q values\n",
    "    from the observations and  converts them into the actions using action_selector\n",
    "    \"\"\"\n",
    "    def __init__(self, dqn_model, action_selector, device=\"cpu\", preprocessor=ptan.agent.default_states_preprocessor):\n",
    "        self.dqn_model = dqn_model\n",
    "        self.action_selector = action_selector\n",
    "        self.preprocessor = preprocessor\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, states, agent_states=None):\n",
    "        if agent_states is None:\n",
    "            agent_states = [None] * len(states)\n",
    "        if self.preprocessor is not None:\n",
    "            states = self.preprocessor(states)\n",
    "            if torch.is_tensor(states):\n",
    "                states = states.to(self.device)\n",
    "        q_v, _ = self.dqn_model(states)\n",
    "        q = q_v.data.cpu().numpy()\n",
    "        actions = self.action_selector(q)\n",
    "        return actions, agent_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent_srg(net, selector, device=device)\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=1)\n",
    "\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=params['replay_size'])\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850: done 1 games, mean reward -21.000, speed 687.81 f/s, eps 0.99\n",
      "1729: done 2 games, mean reward -21.000, speed 726.79 f/s, eps 0.98\n",
      "2640: done 3 games, mean reward -21.000, speed 731.37 f/s, eps 0.97\n",
      "3477: done 4 games, mean reward -21.000, speed 719.52 f/s, eps 0.97\n",
      "4254: done 5 games, mean reward -21.000, speed 710.38 f/s, eps 0.96\n",
      "5187: done 6 games, mean reward -20.833, speed 723.26 f/s, eps 0.95\n",
      "6324: done 7 games, mean reward -20.714, speed 710.31 f/s, eps 0.94\n",
      "7287: done 8 games, mean reward -20.625, speed 723.34 f/s, eps 0.93\n",
      "8220: done 9 games, mean reward -20.667, speed 730.60 f/s, eps 0.92\n",
      "9204: done 10 games, mean reward -20.600, speed 717.78 f/s, eps 0.91\n",
      "10116: done 11 games, mean reward -20.545, speed 487.98 f/s, eps 0.90\n",
      "10979: done 12 games, mean reward -20.583, speed 165.75 f/s, eps 0.89\n",
      "12122: done 13 games, mean reward -20.462, speed 156.69 f/s, eps 0.88\n",
      "13159: done 14 games, mean reward -20.429, speed 165.92 f/s, eps 0.87\n",
      "14115: done 15 games, mean reward -20.400, speed 154.55 f/s, eps 0.86\n",
      "15105: done 16 games, mean reward -20.375, speed 164.49 f/s, eps 0.85\n",
      "15999: done 17 games, mean reward -20.353, speed 157.74 f/s, eps 0.84\n",
      "16970: done 18 games, mean reward -20.389, speed 158.14 f/s, eps 0.83\n",
      "17759: done 19 games, mean reward -20.421, speed 155.45 f/s, eps 0.82\n",
      "18687: done 20 games, mean reward -20.400, speed 160.21 f/s, eps 0.81\n",
      "19526: done 21 games, mean reward -20.429, speed 156.21 f/s, eps 0.80\n",
      "20558: done 22 games, mean reward -20.409, speed 131.21 f/s, eps 0.79\n",
      "21548: done 23 games, mean reward -20.435, speed 136.38 f/s, eps 0.78\n",
      "22642: done 24 games, mean reward -20.417, speed 141.58 f/s, eps 0.77\n",
      "23576: done 25 games, mean reward -20.440, speed 141.60 f/s, eps 0.76\n",
      "24544: done 26 games, mean reward -20.423, speed 140.10 f/s, eps 0.75\n",
      "25572: done 27 games, mean reward -20.444, speed 140.63 f/s, eps 0.74\n",
      "26449: done 28 games, mean reward -20.464, speed 135.82 f/s, eps 0.74\n",
      "27526: done 29 games, mean reward -20.448, speed 139.61 f/s, eps 0.72\n",
      "28613: done 30 games, mean reward -20.467, speed 133.86 f/s, eps 0.71\n",
      "29507: done 31 games, mean reward -20.452, speed 140.74 f/s, eps 0.70\n",
      "30403: done 32 games, mean reward -20.469, speed 130.14 f/s, eps 0.70\n",
      "31379: done 33 games, mean reward -20.455, speed 134.81 f/s, eps 0.69\n",
      "32213: done 34 games, mean reward -20.471, speed 136.09 f/s, eps 0.68\n",
      "33137: done 35 games, mean reward -20.486, speed 139.32 f/s, eps 0.67\n",
      "34038: done 36 games, mean reward -20.472, speed 137.05 f/s, eps 0.66\n",
      "35013: done 37 games, mean reward -20.432, speed 136.58 f/s, eps 0.65\n",
      "36038: done 38 games, mean reward -20.395, speed 129.33 f/s, eps 0.64\n",
      "37330: done 39 games, mean reward -20.333, speed 136.48 f/s, eps 0.63\n",
      "38474: done 40 games, mean reward -20.325, speed 137.01 f/s, eps 0.62\n",
      "39353: done 41 games, mean reward -20.341, speed 140.44 f/s, eps 0.61\n",
      "40238: done 42 games, mean reward -20.357, speed 135.31 f/s, eps 0.60\n",
      "41279: done 43 games, mean reward -20.349, speed 135.07 f/s, eps 0.59\n",
      "42235: done 44 games, mean reward -20.364, speed 139.79 f/s, eps 0.58\n",
      "43144: done 45 games, mean reward -20.378, speed 123.54 f/s, eps 0.57\n",
      "44073: done 46 games, mean reward -20.348, speed 135.99 f/s, eps 0.56\n",
      "45244: done 47 games, mean reward -20.340, speed 135.99 f/s, eps 0.55\n",
      "46278: done 48 games, mean reward -20.354, speed 139.80 f/s, eps 0.54\n",
      "47149: done 49 games, mean reward -20.347, speed 131.19 f/s, eps 0.53\n",
      "48034: done 50 games, mean reward -20.340, speed 135.23 f/s, eps 0.52\n",
      "48916: done 51 games, mean reward -20.353, speed 143.44 f/s, eps 0.51\n",
      "50004: done 52 games, mean reward -20.308, speed 134.52 f/s, eps 0.50\n",
      "50916: done 53 games, mean reward -20.321, speed 141.50 f/s, eps 0.49\n",
      "51764: done 54 games, mean reward -20.333, speed 142.36 f/s, eps 0.48\n",
      "52903: done 55 games, mean reward -20.309, speed 136.35 f/s, eps 0.47\n",
      "54292: done 56 games, mean reward -20.268, speed 141.42 f/s, eps 0.46\n",
      "55306: done 57 games, mean reward -20.246, speed 132.11 f/s, eps 0.45\n",
      "56596: done 58 games, mean reward -20.224, speed 134.95 f/s, eps 0.43\n",
      "57705: done 59 games, mean reward -20.220, speed 143.20 f/s, eps 0.42\n",
      "58846: done 60 games, mean reward -20.217, speed 134.36 f/s, eps 0.41\n",
      "60129: done 61 games, mean reward -20.213, speed 136.44 f/s, eps 0.40\n",
      "61391: done 62 games, mean reward -20.129, speed 139.15 f/s, eps 0.39\n",
      "62626: done 63 games, mean reward -20.111, speed 136.39 f/s, eps 0.37\n",
      "63896: done 64 games, mean reward -20.094, speed 136.25 f/s, eps 0.36\n",
      "65358: done 65 games, mean reward -20.062, speed 136.43 f/s, eps 0.35\n",
      "66336: done 66 games, mean reward -20.076, speed 140.34 f/s, eps 0.34\n",
      "67538: done 67 games, mean reward -20.060, speed 141.67 f/s, eps 0.32\n",
      "68801: done 68 games, mean reward -20.059, speed 136.89 f/s, eps 0.31\n",
      "70358: done 69 games, mean reward -20.000, speed 137.90 f/s, eps 0.30\n",
      "72345: done 70 games, mean reward -19.914, speed 134.69 f/s, eps 0.28\n",
      "73641: done 71 games, mean reward -19.915, speed 159.17 f/s, eps 0.26\n",
      "75068: done 72 games, mean reward -19.903, speed 148.92 f/s, eps 0.25\n",
      "76401: done 73 games, mean reward -19.863, speed 157.75 f/s, eps 0.24\n",
      "77668: done 74 games, mean reward -19.824, speed 158.16 f/s, eps 0.22\n",
      "79208: done 75 games, mean reward -19.787, speed 158.80 f/s, eps 0.21\n",
      "80794: done 76 games, mean reward -19.750, speed 158.97 f/s, eps 0.19\n",
      "82363: done 77 games, mean reward -19.727, speed 158.83 f/s, eps 0.18\n",
      "84021: done 78 games, mean reward -19.705, speed 158.60 f/s, eps 0.16\n",
      "85835: done 79 games, mean reward -19.658, speed 160.48 f/s, eps 0.14\n",
      "87693: done 80 games, mean reward -19.663, speed 155.82 f/s, eps 0.12\n",
      "89285: done 81 games, mean reward -19.642, speed 159.55 f/s, eps 0.11\n",
      "91046: done 82 games, mean reward -19.585, speed 160.27 f/s, eps 0.09\n",
      "92094: done 83 games, mean reward -19.590, speed 159.42 f/s, eps 0.08\n",
      "93667: done 84 games, mean reward -19.571, speed 157.19 f/s, eps 0.06\n",
      "95551: done 85 games, mean reward -19.494, speed 159.70 f/s, eps 0.04\n"
     ]
    }
   ],
   "source": [
    "frame_idx = 0\n",
    "with common.RewardTracker(writer, params['stop_reward']) as reward_tracker: #create a reward tracker object\n",
    "    while True:\n",
    "        frame_idx += 1\n",
    "        # ExperienceReplayBuffer asks the ExperienceSourceFirstLast to iterate by one step to get the next transition\n",
    "        # ExperienceSourceFirstLast feeds observation to obtain action\n",
    "        # Agent calculated Q-values through the NN\n",
    "        # Action selector selects action\n",
    "        # Action is fed into ExperienceSource to obtain reward and next obs\n",
    "        # Buffer stores transition in FIFO order\n",
    "        buffer.populate(1) # iterates ExperienceReplayBuffer by 1 step.\n",
    "                            # this in turn iterates exp_source [ExperienceSourceFirstLast] by one step\n",
    "                            # one single experience step\n",
    "                            # Experience = namedtuple('Experience', ['state', 'action', 'reward', 'done'])\n",
    "                            \n",
    "                            # Class ExperienceSource provides us full subtrajectories of given length as the list of (s, a, r, s') objects.\n",
    "                            # Now it returns single object on every iteration, which is again a namedtuple with the following fields:\n",
    "\n",
    "                            # state: state which we used to decide on action to make TYPE: numpy\n",
    "                            # action: action we've done at this step\n",
    "                            # reward: partial accumulated reward for steps_count (in our case, steps_count=1, so it is equal to immediate reward)\n",
    "                            # last_state: the state we've got after executing the action. If our episode ends, we have None here\n",
    "\n",
    "                            # For every trajectory piece it calculates discounted reward and emits only first and last states and action taken in the first state.\n",
    "        epsilon_tracker.frame(frame_idx)\n",
    "        \n",
    "\n",
    "        new_rewards = exp_source.pop_total_rewards() # get rewards from the episodes\n",
    "        if new_rewards:\n",
    "            if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):\n",
    "                break\n",
    "\n",
    "        if len(buffer) < params['replay_initial']:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(params['batch_size'])\n",
    "        loss_v = common.calc_loss_srg(batch, net, tgt_net.target_model, gamma=params['gamma'], device=device)\n",
    "        if frame_idx % 1E3 == 0:\n",
    "            writer.add_scalar(\"loss\", loss_v, frame_idx)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if frame_idx % params['target_net_sync'] == 0:\n",
    "            tgt_net.sync()\n",
    "            \n",
    "        if frame_idx > 3E6:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "cur_folder = os.getcwd()\n",
    "model_folder = os.path.join(cur_folder,\"models\")\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "model_file = os.path.join(model_folder, (tag + \".pt\"))\n",
    "torch.save(net.state_dict(), model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
