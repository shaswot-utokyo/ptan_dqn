{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DQN using PTAN library\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning) \n",
    "# suppress numpy future warnings \n",
    "# warning created due to issues with tensorflow 1.14\n",
    "\n",
    "\n",
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from lib import dqn_model, common\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable GPU [default:False]\")\n",
    "# parser.add_argument(\"--seed\", type=int, help=\"Set seed [default: 42]\")\n",
    "# parser.add_argument(\"experiment\", help=\"Experiment to run. Specified in ./lib/common.py\")\n",
    "\n",
    "\n",
    "\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "USE_GPU = True#args.cuda\n",
    "USE_CUDA = torch.cuda.is_available() and USE_GPU\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42#args.seed if args.seed else 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "experiment = \"pong\"#args.experiment\n",
    "params = common.HYPERPARAMS[experiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(params['env_name'])\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "\n",
    "tag = params['run_name'] + \"-basic_srg\" + '_'+ str(seed)\n",
    "writer = SummaryWriter(comment=\"-\"+ tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = dqn_model.DQN_A(env.observation_space.shape, \n",
    "                    env.action_space.n).to(device)\n",
    "\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])\n",
    "epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "# EpsilonTracker has only one method frame(frame_idx) which changes the epsilon value \n",
    "# of selector=>ptan.actions.EpsilonGreedyActionSelector depending upon the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent_srg(ptan.agent.BaseAgent):\n",
    "    \"\"\"\n",
    "    DQNAgent is a memoryless DQN agent which calculates Q values\n",
    "    from the observations and  converts them into the actions using action_selector\n",
    "    \"\"\"\n",
    "    def __init__(self, dqn_model, action_selector, device=\"cpu\", preprocessor=ptan.agent.default_states_preprocessor):\n",
    "        self.dqn_model = dqn_model\n",
    "        self.action_selector = action_selector\n",
    "        self.preprocessor = preprocessor\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, states, agent_states=None):\n",
    "        if agent_states is None:\n",
    "            agent_states = [None] * len(states)\n",
    "        if self.preprocessor is not None:\n",
    "            states = self.preprocessor(states)\n",
    "            if torch.is_tensor(states):\n",
    "                states = states.to(self.device)\n",
    "        q_v, _ = self.dqn_model(states)\n",
    "        q = q_v.data.cpu().numpy()\n",
    "        actions = self.action_selector(q)\n",
    "        return actions, agent_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent_srg(net, selector, device=device)\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=1)\n",
    "\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=params['replay_size'])\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850: done 1 games, mean reward -21.000, speed 687.81 f/s, eps 0.99\n",
      "1729: done 2 games, mean reward -21.000, speed 726.79 f/s, eps 0.98\n",
      "2640: done 3 games, mean reward -21.000, speed 731.37 f/s, eps 0.97\n",
      "3477: done 4 games, mean reward -21.000, speed 719.52 f/s, eps 0.97\n",
      "4254: done 5 games, mean reward -21.000, speed 710.38 f/s, eps 0.96\n",
      "5187: done 6 games, mean reward -20.833, speed 723.26 f/s, eps 0.95\n",
      "6324: done 7 games, mean reward -20.714, speed 710.31 f/s, eps 0.94\n",
      "7287: done 8 games, mean reward -20.625, speed 723.34 f/s, eps 0.93\n",
      "8220: done 9 games, mean reward -20.667, speed 730.60 f/s, eps 0.92\n",
      "9204: done 10 games, mean reward -20.600, speed 717.78 f/s, eps 0.91\n",
      "10116: done 11 games, mean reward -20.545, speed 487.98 f/s, eps 0.90\n",
      "10979: done 12 games, mean reward -20.583, speed 165.75 f/s, eps 0.89\n",
      "12122: done 13 games, mean reward -20.462, speed 156.69 f/s, eps 0.88\n",
      "13159: done 14 games, mean reward -20.429, speed 165.92 f/s, eps 0.87\n",
      "14115: done 15 games, mean reward -20.400, speed 154.55 f/s, eps 0.86\n",
      "15105: done 16 games, mean reward -20.375, speed 164.49 f/s, eps 0.85\n",
      "15999: done 17 games, mean reward -20.353, speed 157.74 f/s, eps 0.84\n",
      "16970: done 18 games, mean reward -20.389, speed 158.14 f/s, eps 0.83\n",
      "17759: done 19 games, mean reward -20.421, speed 155.45 f/s, eps 0.82\n",
      "18687: done 20 games, mean reward -20.400, speed 160.21 f/s, eps 0.81\n",
      "19526: done 21 games, mean reward -20.429, speed 156.21 f/s, eps 0.80\n",
      "20558: done 22 games, mean reward -20.409, speed 131.21 f/s, eps 0.79\n",
      "21548: done 23 games, mean reward -20.435, speed 136.38 f/s, eps 0.78\n",
      "22642: done 24 games, mean reward -20.417, speed 141.58 f/s, eps 0.77\n",
      "23576: done 25 games, mean reward -20.440, speed 141.60 f/s, eps 0.76\n",
      "24544: done 26 games, mean reward -20.423, speed 140.10 f/s, eps 0.75\n",
      "25572: done 27 games, mean reward -20.444, speed 140.63 f/s, eps 0.74\n",
      "26449: done 28 games, mean reward -20.464, speed 135.82 f/s, eps 0.74\n",
      "27526: done 29 games, mean reward -20.448, speed 139.61 f/s, eps 0.72\n",
      "28613: done 30 games, mean reward -20.467, speed 133.86 f/s, eps 0.71\n",
      "29507: done 31 games, mean reward -20.452, speed 140.74 f/s, eps 0.70\n",
      "30403: done 32 games, mean reward -20.469, speed 130.14 f/s, eps 0.70\n",
      "31379: done 33 games, mean reward -20.455, speed 134.81 f/s, eps 0.69\n",
      "32213: done 34 games, mean reward -20.471, speed 136.09 f/s, eps 0.68\n",
      "33137: done 35 games, mean reward -20.486, speed 139.32 f/s, eps 0.67\n",
      "34038: done 36 games, mean reward -20.472, speed 137.05 f/s, eps 0.66\n",
      "35013: done 37 games, mean reward -20.432, speed 136.58 f/s, eps 0.65\n",
      "36038: done 38 games, mean reward -20.395, speed 129.33 f/s, eps 0.64\n",
      "37330: done 39 games, mean reward -20.333, speed 136.48 f/s, eps 0.63\n",
      "38474: done 40 games, mean reward -20.325, speed 137.01 f/s, eps 0.62\n",
      "39353: done 41 games, mean reward -20.341, speed 140.44 f/s, eps 0.61\n",
      "40238: done 42 games, mean reward -20.357, speed 135.31 f/s, eps 0.60\n",
      "41279: done 43 games, mean reward -20.349, speed 135.07 f/s, eps 0.59\n",
      "42235: done 44 games, mean reward -20.364, speed 139.79 f/s, eps 0.58\n",
      "43144: done 45 games, mean reward -20.378, speed 123.54 f/s, eps 0.57\n",
      "44073: done 46 games, mean reward -20.348, speed 135.99 f/s, eps 0.56\n",
      "45244: done 47 games, mean reward -20.340, speed 135.99 f/s, eps 0.55\n",
      "46278: done 48 games, mean reward -20.354, speed 139.80 f/s, eps 0.54\n",
      "47149: done 49 games, mean reward -20.347, speed 131.19 f/s, eps 0.53\n",
      "48034: done 50 games, mean reward -20.340, speed 135.23 f/s, eps 0.52\n",
      "48916: done 51 games, mean reward -20.353, speed 143.44 f/s, eps 0.51\n",
      "50004: done 52 games, mean reward -20.308, speed 134.52 f/s, eps 0.50\n",
      "50916: done 53 games, mean reward -20.321, speed 141.50 f/s, eps 0.49\n",
      "51764: done 54 games, mean reward -20.333, speed 142.36 f/s, eps 0.48\n",
      "52903: done 55 games, mean reward -20.309, speed 136.35 f/s, eps 0.47\n",
      "54292: done 56 games, mean reward -20.268, speed 141.42 f/s, eps 0.46\n",
      "55306: done 57 games, mean reward -20.246, speed 132.11 f/s, eps 0.45\n",
      "56596: done 58 games, mean reward -20.224, speed 134.95 f/s, eps 0.43\n",
      "57705: done 59 games, mean reward -20.220, speed 143.20 f/s, eps 0.42\n",
      "58846: done 60 games, mean reward -20.217, speed 134.36 f/s, eps 0.41\n",
      "60129: done 61 games, mean reward -20.213, speed 136.44 f/s, eps 0.40\n",
      "61391: done 62 games, mean reward -20.129, speed 139.15 f/s, eps 0.39\n",
      "62626: done 63 games, mean reward -20.111, speed 136.39 f/s, eps 0.37\n",
      "63896: done 64 games, mean reward -20.094, speed 136.25 f/s, eps 0.36\n",
      "65358: done 65 games, mean reward -20.062, speed 136.43 f/s, eps 0.35\n",
      "66336: done 66 games, mean reward -20.076, speed 140.34 f/s, eps 0.34\n",
      "67538: done 67 games, mean reward -20.060, speed 141.67 f/s, eps 0.32\n",
      "68801: done 68 games, mean reward -20.059, speed 136.89 f/s, eps 0.31\n",
      "70358: done 69 games, mean reward -20.000, speed 137.90 f/s, eps 0.30\n",
      "72345: done 70 games, mean reward -19.914, speed 134.69 f/s, eps 0.28\n",
      "73641: done 71 games, mean reward -19.915, speed 159.17 f/s, eps 0.26\n",
      "75068: done 72 games, mean reward -19.903, speed 148.92 f/s, eps 0.25\n",
      "76401: done 73 games, mean reward -19.863, speed 157.75 f/s, eps 0.24\n",
      "77668: done 74 games, mean reward -19.824, speed 158.16 f/s, eps 0.22\n",
      "79208: done 75 games, mean reward -19.787, speed 158.80 f/s, eps 0.21\n",
      "80794: done 76 games, mean reward -19.750, speed 158.97 f/s, eps 0.19\n",
      "82363: done 77 games, mean reward -19.727, speed 158.83 f/s, eps 0.18\n",
      "84021: done 78 games, mean reward -19.705, speed 158.60 f/s, eps 0.16\n",
      "85835: done 79 games, mean reward -19.658, speed 160.48 f/s, eps 0.14\n",
      "87693: done 80 games, mean reward -19.663, speed 155.82 f/s, eps 0.12\n",
      "89285: done 81 games, mean reward -19.642, speed 159.55 f/s, eps 0.11\n",
      "91046: done 82 games, mean reward -19.585, speed 160.27 f/s, eps 0.09\n",
      "92094: done 83 games, mean reward -19.590, speed 159.42 f/s, eps 0.08\n",
      "93667: done 84 games, mean reward -19.571, speed 157.19 f/s, eps 0.06\n",
      "95551: done 85 games, mean reward -19.494, speed 159.70 f/s, eps 0.04\n",
      "97142: done 86 games, mean reward -19.488, speed 161.02 f/s, eps 0.03\n",
      "98803: done 87 games, mean reward -19.448, speed 160.37 f/s, eps 0.02\n",
      "100954: done 88 games, mean reward -19.420, speed 160.03 f/s, eps 0.02\n",
      "103032: done 89 games, mean reward -19.371, speed 140.69 f/s, eps 0.02\n",
      "104662: done 90 games, mean reward -19.367, speed 136.59 f/s, eps 0.02\n",
      "106534: done 91 games, mean reward -19.308, speed 137.64 f/s, eps 0.02\n",
      "107820: done 92 games, mean reward -19.293, speed 130.55 f/s, eps 0.02\n",
      "110128: done 93 games, mean reward -19.226, speed 141.03 f/s, eps 0.02\n",
      "112457: done 94 games, mean reward -19.191, speed 139.60 f/s, eps 0.02\n",
      "114999: done 95 games, mean reward -19.126, speed 135.13 f/s, eps 0.02\n",
      "117316: done 96 games, mean reward -19.094, speed 138.00 f/s, eps 0.02\n",
      "119479: done 97 games, mean reward -19.072, speed 132.46 f/s, eps 0.02\n",
      "121526: done 98 games, mean reward -19.051, speed 137.99 f/s, eps 0.02\n",
      "123902: done 99 games, mean reward -18.980, speed 138.19 f/s, eps 0.02\n",
      "125673: done 100 games, mean reward -18.980, speed 140.22 f/s, eps 0.02\n",
      "128084: done 101 games, mean reward -18.920, speed 137.93 f/s, eps 0.02\n",
      "130532: done 102 games, mean reward -18.860, speed 131.62 f/s, eps 0.02\n",
      "132334: done 103 games, mean reward -18.830, speed 137.52 f/s, eps 0.02\n",
      "134657: done 104 games, mean reward -18.790, speed 133.50 f/s, eps 0.02\n",
      "136719: done 105 games, mean reward -18.760, speed 139.20 f/s, eps 0.02\n",
      "138414: done 106 games, mean reward -18.690, speed 139.20 f/s, eps 0.02\n",
      "139951: done 107 games, mean reward -18.660, speed 138.88 f/s, eps 0.02\n",
      "141829: done 108 games, mean reward -18.630, speed 139.02 f/s, eps 0.02\n",
      "143754: done 109 games, mean reward -18.580, speed 137.40 f/s, eps 0.02\n",
      "145695: done 110 games, mean reward -18.530, speed 136.67 f/s, eps 0.02\n",
      "146957: done 111 games, mean reward -18.510, speed 138.91 f/s, eps 0.02\n",
      "149455: done 112 games, mean reward -18.470, speed 134.80 f/s, eps 0.02\n",
      "151653: done 113 games, mean reward -18.440, speed 137.09 f/s, eps 0.02\n",
      "154383: done 114 games, mean reward -18.350, speed 136.84 f/s, eps 0.02\n",
      "157164: done 115 games, mean reward -18.300, speed 136.96 f/s, eps 0.02\n",
      "159258: done 116 games, mean reward -18.230, speed 137.49 f/s, eps 0.02\n",
      "161793: done 117 games, mean reward -18.180, speed 135.69 f/s, eps 0.02\n",
      "164218: done 118 games, mean reward -18.100, speed 138.42 f/s, eps 0.02\n",
      "166730: done 119 games, mean reward -17.990, speed 129.99 f/s, eps 0.02\n",
      "169056: done 120 games, mean reward -17.940, speed 138.95 f/s, eps 0.02\n",
      "171294: done 121 games, mean reward -17.890, speed 134.94 f/s, eps 0.02\n",
      "173284: done 122 games, mean reward -17.880, speed 137.32 f/s, eps 0.02\n",
      "175627: done 123 games, mean reward -17.840, speed 135.57 f/s, eps 0.02\n",
      "178628: done 124 games, mean reward -17.740, speed 135.99 f/s, eps 0.02\n",
      "180915: done 125 games, mean reward -17.690, speed 126.50 f/s, eps 0.02\n",
      "183385: done 126 games, mean reward -17.630, speed 137.31 f/s, eps 0.02\n",
      "185316: done 127 games, mean reward -17.590, speed 130.83 f/s, eps 0.02\n",
      "188860: done 128 games, mean reward -17.460, speed 139.44 f/s, eps 0.02\n",
      "191470: done 129 games, mean reward -17.360, speed 140.24 f/s, eps 0.02\n",
      "193724: done 130 games, mean reward -17.310, speed 139.95 f/s, eps 0.02\n",
      "195723: done 131 games, mean reward -17.300, speed 135.75 f/s, eps 0.02\n",
      "198230: done 132 games, mean reward -17.210, speed 130.22 f/s, eps 0.02\n",
      "200697: done 133 games, mean reward -17.190, speed 137.31 f/s, eps 0.02\n",
      "203551: done 134 games, mean reward -17.130, speed 134.95 f/s, eps 0.02\n",
      "207199: done 135 games, mean reward -16.910, speed 140.34 f/s, eps 0.02\n",
      "210532: done 136 games, mean reward -16.830, speed 136.98 f/s, eps 0.02\n",
      "212948: done 137 games, mean reward -16.790, speed 131.24 f/s, eps 0.02\n",
      "215843: done 138 games, mean reward -16.740, speed 136.25 f/s, eps 0.02\n",
      "218473: done 139 games, mean reward -16.680, speed 134.12 f/s, eps 0.02\n",
      "220067: done 140 games, mean reward -16.630, speed 139.46 f/s, eps 0.02\n",
      "223799: done 141 games, mean reward -16.490, speed 134.77 f/s, eps 0.02\n",
      "227247: done 142 games, mean reward -16.340, speed 137.03 f/s, eps 0.02\n",
      "230576: done 143 games, mean reward -16.210, speed 148.00 f/s, eps 0.02\n",
      "234140: done 144 games, mean reward -16.050, speed 155.02 f/s, eps 0.02\n",
      "238761: done 145 games, mean reward -15.800, speed 151.64 f/s, eps 0.02\n",
      "242518: done 146 games, mean reward -15.670, speed 159.93 f/s, eps 0.02\n",
      "245646: done 147 games, mean reward -15.610, speed 163.28 f/s, eps 0.02\n",
      "248450: done 148 games, mean reward -15.530, speed 165.02 f/s, eps 0.02\n",
      "251869: done 149 games, mean reward -15.460, speed 156.68 f/s, eps 0.02\n",
      "255394: done 150 games, mean reward -15.390, speed 156.06 f/s, eps 0.02\n",
      "260395: done 151 games, mean reward -15.190, speed 154.39 f/s, eps 0.02\n",
      "264213: done 152 games, mean reward -14.980, speed 155.06 f/s, eps 0.02\n",
      "266840: done 153 games, mean reward -14.910, speed 163.29 f/s, eps 0.02\n",
      "271639: done 154 games, mean reward -14.710, speed 155.74 f/s, eps 0.02\n",
      "274809: done 155 games, mean reward -14.640, speed 156.46 f/s, eps 0.02\n",
      "278536: done 156 games, mean reward -14.550, speed 160.52 f/s, eps 0.02\n",
      "281244: done 157 games, mean reward -14.490, speed 161.87 f/s, eps 0.02\n",
      "285275: done 158 games, mean reward -14.370, speed 153.01 f/s, eps 0.02\n",
      "288555: done 159 games, mean reward -14.230, speed 150.31 f/s, eps 0.02\n",
      "292443: done 160 games, mean reward -14.050, speed 152.82 f/s, eps 0.02\n",
      "296237: done 161 games, mean reward -13.930, speed 151.57 f/s, eps 0.02\n",
      "300843: done 162 games, mean reward -13.840, speed 154.77 f/s, eps 0.02\n",
      "304375: done 163 games, mean reward -13.720, speed 156.78 f/s, eps 0.02\n",
      "308175: done 164 games, mean reward -13.570, speed 154.26 f/s, eps 0.02\n",
      "311963: done 165 games, mean reward -13.310, speed 156.12 f/s, eps 0.02\n",
      "315307: done 166 games, mean reward -13.210, speed 160.68 f/s, eps 0.02\n",
      "319211: done 167 games, mean reward -13.010, speed 159.57 f/s, eps 0.02\n",
      "322957: done 168 games, mean reward -12.770, speed 157.18 f/s, eps 0.02\n",
      "326782: done 169 games, mean reward -12.730, speed 156.74 f/s, eps 0.02\n",
      "330215: done 170 games, mean reward -12.490, speed 157.30 f/s, eps 0.02\n",
      "333955: done 171 games, mean reward -12.350, speed 160.21 f/s, eps 0.02\n",
      "337846: done 172 games, mean reward -12.100, speed 160.21 f/s, eps 0.02\n",
      "342012: done 173 games, mean reward -11.990, speed 159.13 f/s, eps 0.02\n",
      "345201: done 174 games, mean reward -11.920, speed 159.84 f/s, eps 0.02\n",
      "349824: done 175 games, mean reward -11.820, speed 163.79 f/s, eps 0.02\n",
      "352934: done 176 games, mean reward -11.610, speed 163.79 f/s, eps 0.02\n",
      "356645: done 177 games, mean reward -11.410, speed 163.28 f/s, eps 0.02\n",
      "360522: done 178 games, mean reward -11.190, speed 162.11 f/s, eps 0.02\n",
      "364221: done 179 games, mean reward -11.080, speed 161.72 f/s, eps 0.02\n",
      "368262: done 180 games, mean reward -10.910, speed 159.56 f/s, eps 0.02\n",
      "370742: done 181 games, mean reward -10.610, speed 169.86 f/s, eps 0.02\n",
      "373832: done 182 games, mean reward -10.520, speed 166.87 f/s, eps 0.02\n",
      "377268: done 183 games, mean reward -10.430, speed 171.08 f/s, eps 0.02\n",
      "380995: done 184 games, mean reward -10.320, speed 164.13 f/s, eps 0.02\n",
      "384849: done 185 games, mean reward -10.230, speed 168.50 f/s, eps 0.02\n",
      "388411: done 186 games, mean reward -10.050, speed 163.25 f/s, eps 0.02\n",
      "391937: done 187 games, mean reward -9.860, speed 161.54 f/s, eps 0.02\n",
      "395734: done 188 games, mean reward -9.640, speed 165.85 f/s, eps 0.02\n",
      "398539: done 189 games, mean reward -9.360, speed 156.32 f/s, eps 0.02\n",
      "401153: done 190 games, mean reward -9.050, speed 158.72 f/s, eps 0.02\n",
      "404113: done 191 games, mean reward -8.810, speed 167.34 f/s, eps 0.02\n",
      "407365: done 192 games, mean reward -8.530, speed 167.06 f/s, eps 0.02\n",
      "410844: done 193 games, mean reward -8.470, speed 168.25 f/s, eps 0.02\n",
      "414627: done 194 games, mean reward -8.380, speed 160.14 f/s, eps 0.02\n",
      "419415: done 195 games, mean reward -8.240, speed 162.88 f/s, eps 0.02\n",
      "422477: done 196 games, mean reward -7.990, speed 155.93 f/s, eps 0.02\n",
      "426018: done 197 games, mean reward -7.810, speed 169.82 f/s, eps 0.02\n",
      "430057: done 198 games, mean reward -7.670, speed 169.65 f/s, eps 0.02\n",
      "432241: done 199 games, mean reward -7.380, speed 166.27 f/s, eps 0.02\n",
      "434960: done 200 games, mean reward -7.120, speed 169.86 f/s, eps 0.02\n",
      "437814: done 201 games, mean reward -6.860, speed 169.52 f/s, eps 0.02\n",
      "441013: done 202 games, mean reward -6.610, speed 171.00 f/s, eps 0.02\n",
      "444943: done 203 games, mean reward -6.470, speed 170.77 f/s, eps 0.02\n",
      "447586: done 204 games, mean reward -6.210, speed 169.66 f/s, eps 0.02\n",
      "449802: done 205 games, mean reward -5.860, speed 169.38 f/s, eps 0.02\n",
      "453546: done 206 games, mean reward -5.670, speed 168.32 f/s, eps 0.02\n",
      "455636: done 207 games, mean reward -5.350, speed 168.05 f/s, eps 0.02\n",
      "457587: done 208 games, mean reward -5.010, speed 166.84 f/s, eps 0.02\n",
      "459530: done 209 games, mean reward -4.660, speed 168.31 f/s, eps 0.02\n",
      "461923: done 210 games, mean reward -4.350, speed 168.28 f/s, eps 0.02\n",
      "463722: done 211 games, mean reward -3.990, speed 166.71 f/s, eps 0.02\n",
      "465413: done 212 games, mean reward -3.610, speed 159.70 f/s, eps 0.02\n",
      "467424: done 213 games, mean reward -3.270, speed 170.48 f/s, eps 0.02\n",
      "469088: done 214 games, mean reward -2.960, speed 165.78 f/s, eps 0.02\n",
      "471298: done 215 games, mean reward -2.650, speed 172.40 f/s, eps 0.02\n",
      "473322: done 216 games, mean reward -2.350, speed 171.91 f/s, eps 0.02\n",
      "475842: done 217 games, mean reward -2.040, speed 164.23 f/s, eps 0.02\n",
      "477696: done 218 games, mean reward -1.720, speed 161.80 f/s, eps 0.02\n",
      "479406: done 219 games, mean reward -1.410, speed 165.61 f/s, eps 0.02\n",
      "482178: done 220 games, mean reward -1.160, speed 166.56 f/s, eps 0.02\n",
      "484315: done 221 games, mean reward -0.840, speed 161.82 f/s, eps 0.02\n",
      "486218: done 222 games, mean reward -0.450, speed 157.74 f/s, eps 0.02\n",
      "488180: done 223 games, mean reward -0.090, speed 166.83 f/s, eps 0.02\n",
      "490020: done 224 games, mean reward 0.200, speed 160.40 f/s, eps 0.02\n",
      "491853: done 225 games, mean reward 0.560, speed 160.68 f/s, eps 0.02\n",
      "493937: done 226 games, mean reward 0.860, speed 166.55 f/s, eps 0.02\n",
      "496099: done 227 games, mean reward 1.210, speed 166.01 f/s, eps 0.02\n",
      "498000: done 228 games, mean reward 1.470, speed 165.66 f/s, eps 0.02\n",
      "499956: done 229 games, mean reward 1.760, speed 166.66 f/s, eps 0.02\n",
      "501887: done 230 games, mean reward 2.100, speed 170.47 f/s, eps 0.02\n",
      "503792: done 231 games, mean reward 2.470, speed 166.58 f/s, eps 0.02\n",
      "505647: done 232 games, mean reward 2.780, speed 163.97 f/s, eps 0.02\n",
      "507539: done 233 games, mean reward 3.150, speed 160.16 f/s, eps 0.02\n",
      "509429: done 234 games, mean reward 3.480, speed 164.76 f/s, eps 0.02\n",
      "511362: done 235 games, mean reward 3.640, speed 169.18 f/s, eps 0.02\n",
      "513935: done 236 games, mean reward 3.780, speed 165.94 f/s, eps 0.02\n",
      "515682: done 237 games, mean reward 4.120, speed 160.14 f/s, eps 0.02\n",
      "517756: done 238 games, mean reward 4.410, speed 164.45 f/s, eps 0.02\n",
      "519620: done 239 games, mean reward 4.730, speed 173.88 f/s, eps 0.02\n",
      "521433: done 240 games, mean reward 5.080, speed 164.90 f/s, eps 0.02\n",
      "523188: done 241 games, mean reward 5.360, speed 163.44 f/s, eps 0.02\n",
      "524935: done 242 games, mean reward 5.610, speed 159.53 f/s, eps 0.02\n",
      "526753: done 243 games, mean reward 5.880, speed 171.20 f/s, eps 0.02\n",
      "528750: done 244 games, mean reward 6.130, speed 168.90 f/s, eps 0.02\n",
      "530526: done 245 games, mean reward 6.280, speed 167.62 f/s, eps 0.02\n",
      "532672: done 246 games, mean reward 6.500, speed 163.92 f/s, eps 0.02\n",
      "534442: done 247 games, mean reward 6.850, speed 160.28 f/s, eps 0.02\n",
      "536349: done 248 games, mean reward 7.170, speed 163.15 f/s, eps 0.02\n",
      "538162: done 249 games, mean reward 7.490, speed 159.33 f/s, eps 0.02\n",
      "540048: done 250 games, mean reward 7.810, speed 165.38 f/s, eps 0.02\n",
      "541738: done 251 games, mean reward 8.030, speed 159.02 f/s, eps 0.02\n",
      "543483: done 252 games, mean reward 8.200, speed 170.24 f/s, eps 0.02\n",
      "545523: done 253 games, mean reward 8.530, speed 169.06 f/s, eps 0.02\n",
      "547171: done 254 games, mean reward 8.750, speed 159.04 f/s, eps 0.02\n",
      "549123: done 255 games, mean reward 9.050, speed 164.56 f/s, eps 0.02\n",
      "550753: done 256 games, mean reward 9.350, speed 164.08 f/s, eps 0.02\n",
      "553062: done 257 games, mean reward 9.640, speed 164.18 f/s, eps 0.02\n",
      "554978: done 258 games, mean reward 9.880, speed 162.20 f/s, eps 0.02\n",
      "556624: done 259 games, mean reward 10.150, speed 160.10 f/s, eps 0.02\n",
      "558617: done 260 games, mean reward 10.360, speed 159.36 f/s, eps 0.02\n",
      "560395: done 261 games, mean reward 10.630, speed 171.40 f/s, eps 0.02\n",
      "562866: done 262 games, mean reward 10.830, speed 162.58 f/s, eps 0.02\n",
      "564724: done 263 games, mean reward 11.100, speed 167.36 f/s, eps 0.02\n",
      "566727: done 264 games, mean reward 11.330, speed 159.57 f/s, eps 0.02\n",
      "568453: done 265 games, mean reward 11.450, speed 158.00 f/s, eps 0.02\n",
      "570310: done 266 games, mean reward 11.740, speed 160.75 f/s, eps 0.02\n",
      "572173: done 267 games, mean reward 11.910, speed 158.84 f/s, eps 0.02\n",
      "574090: done 268 games, mean reward 12.060, speed 163.68 f/s, eps 0.02\n",
      "575932: done 269 games, mean reward 12.380, speed 162.74 f/s, eps 0.02\n",
      "577901: done 270 games, mean reward 12.460, speed 161.94 f/s, eps 0.02\n",
      "579816: done 271 games, mean reward 12.710, speed 160.60 f/s, eps 0.02\n",
      "581572: done 272 games, mean reward 12.850, speed 159.20 f/s, eps 0.02\n",
      "583351: done 273 games, mean reward 13.110, speed 163.97 f/s, eps 0.02\n",
      "585407: done 274 games, mean reward 13.380, speed 157.79 f/s, eps 0.02\n",
      "587274: done 275 games, mean reward 13.640, speed 161.07 f/s, eps 0.02\n",
      "588999: done 276 games, mean reward 13.800, speed 153.04 f/s, eps 0.02\n",
      "590755: done 277 games, mean reward 13.980, speed 149.99 f/s, eps 0.02\n",
      "592478: done 278 games, mean reward 14.130, speed 160.79 f/s, eps 0.02\n",
      "594167: done 279 games, mean reward 14.390, speed 159.29 f/s, eps 0.02\n",
      "595934: done 280 games, mean reward 14.630, speed 159.85 f/s, eps 0.02\n",
      "597902: done 281 games, mean reward 14.670, speed 161.24 f/s, eps 0.02\n",
      "599539: done 282 games, mean reward 14.940, speed 156.56 f/s, eps 0.02\n",
      "601625: done 283 games, mean reward 15.230, speed 164.49 f/s, eps 0.02\n",
      "603407: done 284 games, mean reward 15.490, speed 161.44 f/s, eps 0.02\n",
      "605099: done 285 games, mean reward 15.740, speed 171.06 f/s, eps 0.02\n",
      "607094: done 286 games, mean reward 15.920, speed 166.68 f/s, eps 0.02\n",
      "608985: done 287 games, mean reward 16.060, speed 163.39 f/s, eps 0.02\n",
      "611176: done 288 games, mean reward 16.150, speed 160.58 f/s, eps 0.02\n",
      "613061: done 289 games, mean reward 16.220, speed 172.57 f/s, eps 0.02\n",
      "614914: done 290 games, mean reward 16.290, speed 162.99 f/s, eps 0.02\n",
      "616661: done 291 games, mean reward 16.390, speed 170.20 f/s, eps 0.02\n",
      "618404: done 292 games, mean reward 16.490, speed 170.36 f/s, eps 0.02\n",
      "620146: done 293 games, mean reward 16.760, speed 166.43 f/s, eps 0.02\n",
      "621984: done 294 games, mean reward 17.020, speed 161.11 f/s, eps 0.02\n",
      "623627: done 295 games, mean reward 17.220, speed 159.32 f/s, eps 0.02\n",
      "625502: done 296 games, mean reward 17.320, speed 171.60 f/s, eps 0.02\n",
      "627356: done 297 games, mean reward 17.520, speed 166.37 f/s, eps 0.02\n",
      "629348: done 298 games, mean reward 17.730, speed 161.89 f/s, eps 0.02\n",
      "631182: done 299 games, mean reward 17.750, speed 170.77 f/s, eps 0.02\n",
      "632811: done 300 games, mean reward 17.890, speed 172.16 f/s, eps 0.02\n",
      "634737: done 301 games, mean reward 17.970, speed 161.84 f/s, eps 0.02\n",
      "636677: done 302 games, mean reward 18.030, speed 165.82 f/s, eps 0.02\n",
      "Solved in 636677 frames!\n"
     ]
    }
   ],
   "source": [
    "frame_idx = 0\n",
    "with common.RewardTracker(writer, params['stop_reward']) as reward_tracker: #create a reward tracker object\n",
    "    while True:\n",
    "        frame_idx += 1\n",
    "        # ExperienceReplayBuffer asks the ExperienceSourceFirstLast to iterate by one step to get the next transition\n",
    "        # ExperienceSourceFirstLast feeds observation to obtain action\n",
    "        # Agent calculated Q-values through the NN\n",
    "        # Action selector selects action\n",
    "        # Action is fed into ExperienceSource to obtain reward and next obs\n",
    "        # Buffer stores transition in FIFO order\n",
    "        buffer.populate(1) # iterates ExperienceReplayBuffer by 1 step.\n",
    "                            # this in turn iterates exp_source [ExperienceSourceFirstLast] by one step\n",
    "                            # one single experience step\n",
    "                            # Experience = namedtuple('Experience', ['state', 'action', 'reward', 'done'])\n",
    "                            \n",
    "                            # Class ExperienceSource provides us full subtrajectories of given length as the list of (s, a, r, s') objects.\n",
    "                            # Now it returns single object on every iteration, which is again a namedtuple with the following fields:\n",
    "\n",
    "                            # state: state which we used to decide on action to make TYPE: numpy\n",
    "                            # action: action we've done at this step\n",
    "                            # reward: partial accumulated reward for steps_count (in our case, steps_count=1, so it is equal to immediate reward)\n",
    "                            # last_state: the state we've got after executing the action. If our episode ends, we have None here\n",
    "\n",
    "                            # For every trajectory piece it calculates discounted reward and emits only first and last states and action taken in the first state.\n",
    "        epsilon_tracker.frame(frame_idx)\n",
    "        \n",
    "\n",
    "        new_rewards = exp_source.pop_total_rewards() # get rewards from the episodes\n",
    "        if new_rewards:\n",
    "            if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):\n",
    "                break\n",
    "\n",
    "        if len(buffer) < params['replay_initial']:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(params['batch_size'])\n",
    "        loss_v = common.calc_loss_srg(batch, net, tgt_net.target_model, gamma=params['gamma'], device=device)\n",
    "        if frame_idx % 1E3 == 0:\n",
    "            writer.add_scalar(\"loss\", loss_v, frame_idx)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if frame_idx % params['target_net_sync'] == 0:\n",
    "            tgt_net.sync()\n",
    "            \n",
    "        if frame_idx > 3E6:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "cur_folder = os.getcwd()\n",
    "model_folder = os.path.join(cur_folder,\"models\")\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "model_file = os.path.join(model_folder, (tag + \".pt\"))\n",
    "torch.save(net.state_dict(), model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
